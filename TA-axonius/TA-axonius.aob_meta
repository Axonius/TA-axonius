{"basic_builder": {"appname": "TA-axonius", "friendly_name": "Axonius Add-On for Splunk", "version": "1.2.7", "author": "Axonius", "description": "The Axonius Splunk Technology Add-on is designed to help your organization offload data from Axonius into Splunk. The TA can be configured to use the existing saved queries already available in your environment.", "theme": "#FFFFFF", "large_icon": "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", "small_icon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAADxklEQVRYR72Ye2iOURzHzxkKDcPcSvbKNZNyt1wit8iKkEtIiS1Mc4tMzFBuTUpsr8XcUvjHKNcSktIkZkTSO3+45Jbb2LvXHt9ve169OzvPe17P3me/+ux53nP5ne9+zznn+Z1HilqbBwpAa3AfrAF37TqvLiPheC9IAz/ACnBC4s9scFYZtQq/u4KPHqlJgt+3oLnify4FPQN9NQNvRNlujwSthd99Gt8BCrIcBs1D+VaPBO2A3xydbwo6DDI1ladQttAjQafhd77GdxEFtQV3QKrS4Cd+81mH4iyqCfx9Aa0Uv5w6oygobOm4GQgSwCxb4BVcWR4vUU3hqwRMAeXgnD0478/zPlKQGoj1KNgDqsFN8Ah8BUXgfYxR64R2S0AyGAyGAa6sLWC7zkc0QWz/CnRXOjJa48Ftg6gxqL8BmintKvDb59TXJGgXOm7QdL6FsrEGQVdRP0nThlsJtxStmQRx2edqetagjJMzmjGSujb0t82toEXoeFzT+SXKehkEPUG9unLZJQP43QpiP66AfoqDyfh9zSBooqbNC5T1idbP9MjYt6X92Jbieg/sB9cNYsLVE3CzGowGR+xH9a2hghz7W1k9O4hgcBN2Dz6GIN5CBcIK5Un/m8oYBddrFkuEtL5rxVRznnSs08CyykUwYYgsDvx2I8q9oEzfAWFZq/RqxU7pr9jcuIIyumFZS4elL8tkYWBAIwtK4TxpoR/UKpWFr4c2rqBlKfl4E3IFaUzmIUKucin3cyjblyQqa0qFlD0URQ+FlTxc+h/wpfzf5loQR7KWpyaK0PcckSBniBpRhYiVQAyWvTsx9BmLIE5OvmT5ovwMVgL1UOAUCeZS+cAHLgCmrc+jhc0kiMnaB9BOcTInBlEUw2Qs0vgPMTdyyuONEeIOzPOaahRZd0Os3yaAohRN3yyUHXSKkilCTunHHzhkOhrNPEk/eLJcpxn1MsqmGgRdRP00TRuex5gea80UoXfoxbw40jgPxoHHBkGDUM/MMlFpx3y8sxtBTKKYctBBMfgFeLQ+BBwnpTIQHyvP7DxOdQEzASd1IdCdBf9Nar4COIHZmBsdk6v24CjgqSGedgzOFgNGn9sHI054YA3xkfUGTLzUpf3JFhhPMWFfjDT/4Ujjyk2joJNggWZUL6ITHoZ5OvN11c5E+9iQi9aOp4MGho1+eVisZxTExzVCU8dPJtz2vbBsOGVurloZBfEEwTN8pHGS8YMVV5YXxkXED1ZtFOfp4X2IonLtSPEzDM/dPLJ4af3tcabj+hTwhHzpLxgRy+2ycTB3AAAAAElFTkSuQmCC", "visible": false, "tab_version": "4.1.1", "tab_build_no": "0", "build_no": 5}, "data_input_builder": {"datainputs": [{"index": "default", "sourcetype": "axonius_saved_query", "interval": "86400", "use_external_validation": true, "streaming_mode_xml": true, "name": "axonius_saved_query", "title": "Axonius Saved Query", "description": "", "type": "customized", "parameters": [{"name": "api_host", "label": "Axonius Host", "help_string": "The URL of the Axonius web host", "required": true, "format_type": "text", "default_value": "", "placeholder": "https://axonius.example.com", "type": "text", "value": "https://10.20.0.103"}, {"name": "entity_type", "label": "Entity Type", "help_string": "The entity type of the saved query, either devices or users", "required": true, "possible_values": [{"value": "devices", "label": "Devices"}, {"value": "users", "label": "Users"}], "format_type": "dropdownlist", "default_value": "devices", "placeholder": "", "type": "dropdownlist", "value": "devices"}, {"name": "saved_query", "label": "Saved Query", "help_string": "The name of the saved query", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "Windows Servers"}, {"name": "page_size", "label": "Page Size", "help_string": "The number of asset entities to fetch during each API call, higher is quicker while lower takes less memory", "required": true, "format_type": "text", "default_value": "1000", "placeholder": "", "type": "text", "value": "10"}, {"name": "standoff_ms", "label": "API Standoff (milliseconds)", "help_string": "The number of milliseconds to wait between successive API calls", "required": true, "format_type": "text", "default_value": "0", "placeholder": "", "type": "text", "value": "0"}, {"name": "dynamic_field_mapping", "label": "Dynamic Field Mapping", "help_string": "Rename fields using a JSON-formatted string, renaming occurs prior to data ingest", "required": false, "format_type": "text", "default_value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}", "placeholder": "", "type": "text", "value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}"}, {"name": "shorten_field_names", "label": "Shorten Field Names", "help_string": "Truncate the field name prefix, if applicable (specific_data.data, adapters_data)", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "incremental_data_ingest", "label": "Incremental Data Ingest", "help_string": "Include only the entities that have a fetch timer newer than last collection", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "enforce_ssl_validation", "label": "Enforce SSL Validation", "help_string": "Enforce SSL certificate validation (the Splunk server's global certificate trust will be used if CA Bundle Path is left blank)", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "enable_include_details", "label": "Enable \"Include Details\"", "help_string": "Enable extra information to be returned in the result set that marries fields to their source adapter.\n", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "ssl_certificate_path", "label": "CA Bundle Path", "help_string": "The filesystem path to the CA bundle used for SSL certificate validation", "required": false, "format_type": "text", "default_value": "", "placeholder": "Path to CA bundle (Examples: C:/Certs/ca_bundle.pem or /home/splunk/ca_bundle.pem)", "type": "text", "value": ""}], "data_inputs_options": [{"type": "customized_var", "name": "api_host", "title": "Axonius Host", "description": "The URL of the Axonius web host", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": "https://axonius.example.com"}, {"type": "customized_var", "name": "entity_type", "title": "Entity Type", "description": "The entity type of the saved query, either devices or users", "required_on_edit": false, "required_on_create": true, "possible_values": [{"value": "devices", "label": "Devices"}, {"value": "users", "label": "Users"}], "format_type": "dropdownlist", "default_value": "devices", "placeholder": ""}, {"type": "customized_var", "name": "saved_query", "title": "Saved Query", "description": "The name of the saved query", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "page_size", "title": "Page Size", "description": "The number of asset entities to fetch during each API call, higher is quicker while lower takes less memory", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "1000", "placeholder": ""}, {"type": "customized_var", "name": "standoff_ms", "title": "API Standoff (milliseconds)", "description": "The number of milliseconds to wait between successive API calls", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "0", "placeholder": ""}, {"type": "customized_var", "name": "dynamic_field_mapping", "title": "Dynamic Field Mapping", "description": "Rename fields using a JSON-formatted string, renaming occurs prior to data ingest", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}", "placeholder": ""}, {"type": "customized_var", "name": "shorten_field_names", "title": "Shorten Field Names", "description": "Truncate the field name prefix, if applicable (specific_data.data, adapters_data)", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "incremental_data_ingest", "title": "Incremental Data Ingest", "description": "Include only the entities that have a fetch timer newer than last collection", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "enforce_ssl_validation", "title": "Enforce SSL Validation", "description": "Enforce SSL certificate validation (the Splunk server's global certificate trust will be used if CA Bundle Path is left blank)", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "enable_include_details", "title": "Enable \"Include Details\"", "description": "Enable extra information to be returned in the result set that marries fields to their source adapter.\n", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "ssl_certificate_path", "title": "CA Bundle Path", "description": "The filesystem path to the CA bundle used for SSL certificate validation", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": "Path to CA bundle (Examples: C:/Certs/ca_bundle.pem or /home/splunk/ca_bundle.pem)"}], "code": "# encoding = utf-8\n\nimport datetime\nimport json\nimport re\nimport requests\nimport time\n\nfrom urllib.parse import urlparse\n\n\nclass Config:\n    supported_minimum_version: str = \"4.4.0\"\n    retry_standoff: list = [0, 5, 10, 15, 30, 60]\n    request_timeout: int = 900\n\n\nclass API:\n    def __init__(self, url, api_key, api_secret, verify=False, timeout=900):\n        self._url = url\n        self._api_key = api_key\n        self._api_secret = api_secret\n        self._verify = verify\n        self._timeout = timeout\n\n    def _rest_base(self, method, api_endpoint, data=None, params=None, headers={}):\n        requests_method = getattr(requests, method)\n        exception = None\n        req = None\n\n        try:\n            headers['api-key'] = self._api_key\n            headers['api-secret'] = self._api_secret\n\n            if self._verify:\n                req = requests_method(f\"{self._url}{api_endpoint}\", timeout=self._timeout, params=params,\n                                      data=json.dumps(data), headers=headers)\n            else:\n                req = requests_method(f\"{self._url}{api_endpoint}\", timeout=self._timeout, params=params,\n                                      data=json.dumps(data), headers=headers, verify=self._verify)\n\n        except Exception as e:\n            exception = e\n\n        req_status_code = None\n\n        if req is not None:\n            req_status_code = req.status_code\n\n        req_json = {\"data\": \"\"}\n\n        if req is not None:\n            req_json = req.json()\n\n        return req_status_code, req_json, exception\n\n    def get(self, api_endpoint, data=None, params=None, headers={}):\n        return self._rest_base(\"get\", api_endpoint, data=data, params=params, headers=headers)\n\n    def post(self, api_endpoint, data=None, params=None, headers={}):\n        return self._rest_base(\"post\", api_endpoint, data=data, params=params, headers=headers)\n\n\nclass Metadata:\n    def __init__(self, api):\n        self._api = api\n        self._api_endpoint = \"/api/settings/metadata\"\n\n    def get_version(self):\n        status, response, exception = self._api.get(self._api_endpoint)\n\n        if status == 200 and response is not None and exception is None:\n            return response[\"Installed Version\"]\n        else:\n            raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n\nclass Lifecycle:\n    def __init__(self, api):\n        self._api = api\n        self._api_endpoint = \"/api/dashboard/lifecycle\"\n        self._response = None\n        self._sub_phases = {}\n        self.status = None\n\n    def update(self):\n        status, response, exception = self._api.get(self._api_endpoint)\n\n        if status == 200 and response is not None and exception is None:\n            self._response = response\n            status = self._response[\"data\"][\"attributes\"][\"status\"]\n            self.status = True if \"done\" not in status else False\n\n            for sub_phase in self._response[\"data\"][\"attributes\"][\"sub_phases\"]:\n                self._sub_phases[sub_phase[\"name\"].lower()] = True if sub_phase[\"status\"] == 1 else False\n        else:\n            raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n    def discovery_is_running(self):\n        if not bool(self._sub_phases):\n            self.update()\n\n        return self.status\n\n    def correlation_is_complete(self):\n        if not bool(self._sub_phases):\n            self.update()\n\n        return self._sub_phases[\"post_correlation\"]\n\n\nclass SavedQueries:\n    def __init__(self, api, base_api_endpoint):\n        self._api = api\n        self._api_endpoint = base_api_endpoint\n        self._queries = {}\n\n    def get_attributes_by_name(self, query_name):\n\n        if not bool(self._queries):\n            status, response, exception = self._api.get(f\"{self._api_endpoint}/views/saved\")\n\n            if exception is not None:\n                raise Exception(exception)\n\n            for query in response[\"data\"]:\n                self._queries[query[\"attributes\"][\"name\"]] = query[\"attributes\"][\"uuid\"]\n\n        if query_name not in self._queries.keys():\n            raise Exception(f\"Critical error: The saved query '{query_name}' does not exist\")\n        else:\n            uuid = self._queries[query_name]\n\n        for query in response[\"data\"]:\n            if query[\"attributes\"][\"uuid\"] == uuid:\n                query_filter = query[\"attributes\"][\"view\"][\"query\"].get(\"filter\")\n                query_fields = query[\"attributes\"][\"view\"].get(\"fields\")\n                query_column_filters = query[\"attributes\"][\"view\"].get(\"colFilters\")\n\n        return uuid, query_filter, query_fields, query_column_filters\n\n\nclass EntitySearch:\n    def __init__(self, api, entity_type, page_size=1000, include_details=True,\n                 logger_callback=None):\n\n        self._api = api\n        self._api_endpoint = f\"/api/{entity_type}\"\n        self._page_size = page_size\n        self._include_details = include_details\n        self._cursor = None\n        self._logger_callback = logger_callback\n        self._uuid = None\n        self._query_filter = None\n        self._query_fields = None\n        self._query_column_filters = None\n\n    def _log(self, msg):\n        if self._logger_callback is not None:\n            self._logger_callback(msg)\n\n    def connection_test(self) -> None:\n        data = {\n            \"data\": {\n                \"type\": \"entity_request_schema\",\n                \"attributes\": {\n                    \"page\": {\n                        \"limit\": 1\n                    },\n                    \"use_cache_entry\": False,\n                    \"always_cached_query\": False,\n                    \"get_metadata\": True,\n                    \"include_details\": True\n                }\n            }\n        }\n\n        status, response, exception = self._api.post(self._api_endpoint, data)\n        if not (status == 200 and response is not None and exception is None):\n            raise Exception(f\"Critical Error! Status Code: {status}\\tException: {exception}\")\n\n\n    def execute_saved_query(self, name, standoff=0, shorten_field_names=False, dynamic_field_mapping={},\n                            incremental_ingest=False, include_auids=False, truncate_fields=[], batch_callback=None):\n        try:\n            ax_saved_queries = SavedQueries(self._api, self._api_endpoint)\n\n            if self._uuid is None or self._query_filter is None or self._query_fields is None:\n                self._uuid, self._query_filter, self._query_fields, self._query_column_filters = ax_saved_queries.get_attributes_by_name(name)\n\n            if incremental_ingest:\n                if \"specific_data.data.fetch_time\" not in self._query_fields:\n                    self._query_fields.append(\"specific_data.data.fetch_time\")\n\n            if include_auids:\n                if \"internal_axon_id\" not in self._query_fields:\n                    self._query_fields.append(\"internal_axon_id\")\n\n            response = {\"data\": \"init\"}\n            entities = []\n            entity_count = 0\n\n            while response[\"data\"]:\n                data = {\n                    \"data\": {\n                        \"type\": \"entity_request_schema\",\n                        \"attributes\": {\n                            \"use_cache_entry\": False,\n                            \"always_cached_query\": False,\n                            \"filter\": self._query_filter,\n                            \"fields\": {\n                                \"devices\": self._query_fields\n                            },\n                            \"page\": {\n                                \"limit\": self._page_size\n                            },\n                            \"get_metadata\": True,\n                            \"include_details\": self._include_details,\n                            \"use_cursor\": True,\n                            \"cursor_id\": self._cursor\n                        }\n                    }\n                }\n\n                if self._query_column_filters:\n                    data[\"data\"][\"attributes\"][\"field_filters\"] = self._query_column_filters\n\n                status, response, exception = self._api.post(self._api_endpoint, data=data)\n\n                if status == 200 and response is not None and exception is None:\n                    if \"meta\" in response:\n                        self._cursor = response[\"meta\"][\"cursor\"]\n\n                        for device in response[\"data\"]:\n                            entity_row = {}\n\n                            for field in list(device['attributes'].keys()):\n                                field_name = field\n\n                                if shorten_field_names:\n                                    field_name = field.replace(\"specific_data.data.\", \"\").replace(\"adapters_data.\", \"\")\n\n                                if field_name in dynamic_field_mapping.keys():\n                                    field_name = dynamic_field_mapping[field_name]\n\n                                entity_row[field_name] = device['attributes'][field]\n\n                            entities.append(entity_row)\n\n                    else:\n                        response = {\"data\": None}\n\n                else:\n                    raise Exception(f\"Critical Error! Status Code: '{status}' Exception: '{exception}'\")\n\n                if standoff > 0:\n                    time.sleep(standoff)\n\n                if batch_callback is not None:\n                    if len(entities) > 0:\n                        batch_callback(entities)\n                        entity_count += len(entities)\n                        entities = []\n\n        except Exception as ex:\n            raise Exception(f\"Critical Error! Status Code: Exception: {ex}\")\n\n\nclass EventWriter:\n    def __init__(self, incremental_data_ingest=False, remove_fetch_time_field=False, fetch_time_field_name=None,\n                 checkpoint=None, host=None, source=None, index=None, sourcetype=None, helper=None, event_writer=None):\n        self._incremental_data_ingest = incremental_data_ingest\n        self._remove_fetch_time_field = remove_fetch_time_field\n        self._fetch_time_field_name = fetch_time_field_name\n        self._checkpoint = checkpoint\n        self._host = host\n        self._source = source\n        self._index = index\n        self._sourcetype = sourcetype\n        self._helper = helper\n        self._event_writer = event_writer\n        self._checkpoint = checkpoint\n        self._entity_count = 0\n        self._entity_ids = []\n        self._page = 0\n        self._events_written = 0\n\n    def process_batch(self, entities):\n        # Update entity count\n        self._entity_count += len(entities)\n\n        # Increment page number\n        self._page += 1\n\n        # Log page number and size\n        self._helper.log_info(f\"\"\"Input '{self._helper.get_arg('name')}' - STATS - Processing page {self._page}, \n                            size {len(entities)}\"\"\")\n\n        # Process each entity\n        for entity in entities:\n            if self._helper.get_arg('name') is None:\n                self._entity_ids.append(entity[\"internal_axon_id\"])\n\n            if self._incremental_data_ingest:\n                # Create a timestamp from the devices fetch_time field\n                entity_fetch_time = datetime.datetime.strptime(entity[self._fetch_time_field_name],\n                                                               \"%a, %d %b %Y %H:%M:%S %Z\").timestamp()\n\n                # Remove the fetch_time field if it was not part of the saved query's query_field definition\n                if self._remove_fetch_time_field:\n                    entity.pop(self._fetch_time_field_name)\n\n                # Create event\n                event = self._helper.new_event(source=self._source, host=self._host, index=self._index,\n                                               sourcetype=self._sourcetype, data=json.dumps(entity))\n\n                # Add event if no checkpoint is defined yet, or if fetch time is greater than the checkpoint time\n                if self._checkpoint is None:\n                    self._event_writer.write_event(event)\n                    self._events_written += 1\n                elif entity_fetch_time > self._checkpoint:\n                    self._event_writer.write_event(event)\n                    self._events_written += 1\n            else:\n                # Create event\n                event = self._helper.new_event(source=self._source, host=self._host, index=self._index,\n                                               sourcetype=self._sourcetype, data=json.dumps(entity))\n\n                # Write event\n                self._event_writer.write_event(event)\n                self._events_written += 1\n\n    def get_entity_count(self):\n        return self._entity_count\n\n    def get_events_written(self):\n        return self._events_written\n\n    def get_internal_axon_id_unique_count(self):\n        return len(set(self._entity_ids))\n\n\ndef validate_input(helper, definition):\n    # get Axonius configuration\n    api_host = definition.parameters.get('api_host', str)\n    api_key = definition.parameters.get('api_key', \"\")\n    api_secret = definition.parameters.get('api_secret', \"\")\n\n    # get selected saved query info\n    entity_type = definition.parameters.get('entity_type', str)\n    saved_query = definition.parameters.get('saved_query', str)\n\n    # get extra options\n    page_size = definition.parameters.get('page_size', str)\n    api_standoff = definition.parameters.get('standoff_ms', str)\n    ssl_certificate_path = definition.parameters.get('ssl_certificate_path', \"\")\n    enforce_ssl_validation = definition.parameters.get('enforce_ssl_validation')\n\n    if int(page_size) < 1:\n        raise ValueError('\"Page Size\" must be an integer greater than 0')\n\n    if int(api_standoff) < 0:\n        raise ValueError(\n            '\"API Standoff\" must be an integer greater or equal to 0'\n            )\n\n    url_parts = urlparse(api_host)\n    if not all([getattr(url_parts, attrs) for attrs in ('scheme', 'netloc')]):\n        raise ValueError('\"The provided URL is invalid.\"')\n\n    if not api_host.startswith('https://'):\n        raise ValueError('\"URL\" must start with https://')\n\n    # Create api object\n    try:\n        verify = True\n\n        helper.log_info(f\"enforce_ssl_validation: {enforce_ssl_validation}\")\n\n        if str(enforce_ssl_validation).lower() not in [\"1\", \"true\"]:\n            verify = False\n\n        helper.log_info(f\"verify: {verify}\")\n\n        if ssl_certificate_path is not None:\n            if len(ssl_certificate_path) > 0:\n                verify = ssl_certificate_path\n\n        api = API(api_host, str(api_key), str(api_secret), verify)\n        search = EntitySearch(api, \"devices\", 1)\n        search.connection_test()\n\n    except Exception as ex:\n        helper.log_info(ex)\n\n        if \"Could not find a suitable TLS CA certificate bundle\" in str(ex):\n            raise ValueError(\"Critical Error, check CA Bundle Path exists and the splunk user has proper permissions\")\n        elif \"SSLCertVerificationError\" in str(ex) or \"Could not find a suitable TLS CA certificate bundle\" in str(ex):\n            raise ValueError(\n                \"The Axonius host fails SSL verification, please review your SSL certificate validation settings\")\n        elif \"Status Code: 401\" not in str(ex):\n            raise ValueError(f\"Critical Error: {ex}\")\n\n    pass\n\n\ndef collect_events(helper, ew):\n    # Retrieve checkpoint\n    checkpoint_name = f\"checkpoint_{helper.get_arg('name')}_{helper.get_arg('entity_type')}_{helper.get_arg('saved_query')}\"\n\n    # get Axonius configuration\n    opt_api_host = helper.get_arg('api_host')\n\n    opt_api_key = helper.get_global_setting('api_key')\n    opt_api_secret = helper.get_global_setting('api_secret')\n\n    # get selected saved query info\n    opt_entity_type = helper.get_arg('entity_type')\n    opt_saved_query = helper.get_arg('saved_query')\n\n    # get extra options\n    opt_page_size = helper.get_arg('page_size')\n    opt_shorten_field_names = helper.get_arg('shorten_field_names')\n    opt_incremental_data_ingest = helper.get_arg('incremental_data_ingest')\n    opt_standoff_ms = helper.get_arg('standoff_ms')\n    opt_field_mapping = helper.get_arg('dynamic_field_mapping')\n    opt_ssl_certificate_path = helper.get_arg('ssl_certificate_path')\n    opt_enforce_ssl_validation = helper.get_arg('enforce_ssl_validation')\n    opt_enable_include_details = helper.get_arg('enable_include_details')\n\n    # Logging functions\n    def log_info(msg):\n        helper.log_info(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    def log_warning(msg):\n        helper.log_warning(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    def log_error(msg):\n        helper.log_error(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    def log_critical(msg):\n        helper.log_critical(f\"Input '{helper.get_arg('name')}' - {msg}\")\n\n    # Log input variables\n    log_info(f\"VARS - Axonius Host: {opt_api_host}\")\n    log_info(f\"VARS - Entity type: {opt_entity_type}\")\n    log_info(f\"VARS - Saved query: {opt_saved_query}\")\n    log_info(f\"VARS - Page size: {opt_page_size}\")\n    log_info(f\"VARS - Shorten field names: {opt_shorten_field_names}\")\n    log_info(f\"VARS - Incremental data ingest: {opt_incremental_data_ingest}\")\n    log_info(f\"VARS - API standoff (ms): {opt_standoff_ms}\")\n    log_info(f\"VARS - Field Mapping: {opt_field_mapping}\")\n    log_info(f\"VARS - Enforce SSL validation: {opt_enforce_ssl_validation}\")\n    log_info(f\"VARS - Enable include details: {opt_enable_include_details}\")\n    log_info(f\"VARS - CA bundle path: {opt_ssl_certificate_path}\")\n\n    include_auids = True if helper.get_arg('name') is None else False\n    critical_error = False\n\n    # Set verify to True/False\n    verify = opt_enforce_ssl_validation\n\n    # Change the value of verify to the path of the ca_bundle if specified\n    if opt_ssl_certificate_path:\n        if len(opt_ssl_certificate_path) > 0:\n            verify = opt_ssl_certificate_path\n\n    # The host field will be used to set the source host in search\n\n    # Pull out just the host information from the Host\n    host = urlparse(opt_api_host).hostname\n\n    if helper.get_global_setting('api_secret'):\n        timeout = int(helper.get_global_setting('https_request_timeout'))\n    else:\n        timeout = Config.request_timeout if helper.get_arg('name') is not None else 5\n\n    retry_standoff = Config.retry_standoff if helper.get_arg('name') is not None else [0, 3, 3, 3]\n\n    # Create an API object for REST calls\n    api = API(opt_api_host, opt_api_key, opt_api_secret, verify, timeout=timeout)\n\n    # Create EntitySearch object with entity type and page size\n    search = EntitySearch(api, opt_entity_type, opt_page_size, \n                          opt_enable_include_details, log_info)\n\n    log_info(checkpoint_name)\n\n    # Load the input's checkpoint data\n    checkpoint = helper.get_check_point(checkpoint_name)\n\n    if checkpoint is not None:\n        log_info(f\"VARS - Check point: {checkpoint_name}\")\n\n    # Default dynamic field names to an empty dict in case opt_field_mapping is empty\n    dynamic_field_names = {}\n\n    # Use dynamic mapping if specified\n    if opt_field_mapping is not None:\n        if len(opt_field_mapping) > 0:\n            try:\n                dynamic_field_names = json.loads(opt_field_mapping)\n            except Exception as ex:\n                pass\n\n    # Retry variables\n    fetch_complete = False\n    exception_thrown = False\n    max_retries = len(retry_standoff)\n    entity_count = 0\n    retries = 0\n    version = None\n    event_writer = None\n    lifecycle_complete = False\n\n    # Set the fetch_time field name, take into account the use of shorten field name\n    fetch_time_field_name = \"fetch_time\" if opt_shorten_field_names else \"specific_data.data.fetch_time\"\n\n    while retries < max_retries and not critical_error and not fetch_complete:\n        try:\n            if version is None:\n                # Get the raw Axonius version from the metadata endpoint\n                metadata = Metadata(api)\n                version = metadata.get_version()\n\n                # Pull out just the host information from the Host\n                match = re.match(\"(\\d+\\_\\d+\\_\\d+)(?:_RC\\d+)\", version)\n\n                # Only set host if the regex exists, match should never be None.\n                if match is not None:\n                    version = match.groups()[0].replace(\"_\", \".\")\n\n                log_info(f\"STATS - Version: {version}\")\n\n                # Turn versions into tuples for equality comparison\n                tup_version = tuple(map(int, (version.split(\".\"))))\n                tup_supported_version = tuple(map(int, (Config.supported_minimum_version.split(\".\"))))\n\n                # If the current version is less than supported, throw a critical exception\n                if tup_version < tup_supported_version:\n                    raise Exception(\"UnsupportedVersion\")\n\n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n\n            if not lifecycle_complete:\n                # Check if a discovery is running and correlation hasn't complete, warn customer if true\n                lifecycle = Lifecycle(api)\n\n                if lifecycle.discovery_is_running() and not lifecycle.correlation_is_complete():\n                    log_warning(f\"Warning: Fetch started while correlation was not complete.\")\n\n                lifecycle_complete = True\n\n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n\n            if not event_writer:\n                # Get definition of query_fields, used to check if the fetch_time field should be removed\n                api_endpoint = f\"/api/{opt_entity_type}\"\n                ax_saved_queries = SavedQueries(api, api_endpoint)\n                uuid, query_filter, query_fields, query_column_filters = ax_saved_queries.get_attributes_by_name(opt_saved_query)\n\n                # Default remove fetch time to true\n                remove_fetch_time_field = True\n\n                # Look for fetch_time in the query_fields definition of the specified saved query\n                if opt_shorten_field_names:\n                    if fetch_time_field_name in query_fields:\n                        remove_fetch_time_field = False\n\n                # Create EventWriter instance to process batches\n                event_writer = EventWriter(incremental_data_ingest=opt_incremental_data_ingest,\n                                           remove_fetch_time_field=remove_fetch_time_field,\n                                           fetch_time_field_name=fetch_time_field_name, checkpoint=checkpoint,\n                                           host=host, source=helper.get_arg('name'), index=helper.get_output_index(),\n                                           sourcetype=helper.get_sourcetype(), helper=helper, event_writer=ew)\n\n                # Reset retries and exception_thrown\n                retries = 0\n                exception_thrown = False\n\n            # Grab entity from the saved search\n            search.execute_saved_query(opt_saved_query, int(opt_standoff_ms) / 1000, opt_shorten_field_names,\n                                       dynamic_field_names, incremental_ingest=opt_incremental_data_ingest,\n                                       include_auids=include_auids, batch_callback=event_writer.process_batch)\n\n            # Get Stats\n            entity_count = event_writer.get_entity_count()\n            events_written = event_writer.get_events_written()\n\n            # Fetch is complete, see below for consistency checks if an exception was thrown during fetch\n            fetch_complete = True\n\n            # Log stats\n            log_info(f\"STATS - Total entities returned: {entity_count}\")\n            log_info(f\"STATS - Total events written: {events_written}\")\n\n            # Sanity check for unique ids, the number needs to match entity_count\n            if helper.get_arg('name') is None:\n                log_info(f\"STATS - Total unique ids: {event_writer.get_internal_axon_id_unique_count()}\")\n        except Exception as ex:\n            # Die if running an unsupported version of Axonius, or log the error and track for retry purposes\n            if \"UnsupportedVersion\" in str(ex):\n                critical_error = True\n            else:\n                log_error(f\"ERR - Error '{ex}'\")\n                exception_thrown = True\n\n        if critical_error:\n            log_critical(\n                f\"Critical Error: Axonius version {version} is unsupported, the minimum version is {Config.supported_minimum_version}\")\n        elif exception_thrown and not fetch_complete:\n            # Increment retry counter\n            retries += 1\n\n            if retries < max_retries:\n                # Log retry number and display the standoff\n                log_info(f\"COLL - Retry {retries} sleeping for {retry_standoff[retries]} seconds, then retrying\")\n\n                # Sleep the process and then retry\n                time.sleep(retry_standoff[retries])\n            else:\n                # Log no devices after max retries\n                log_critical(f\"Critical Error: Unable to complete fetch due to unrecoverable errors.\")\n        elif exception_thrown and fetch_complete:\n            # Log recovered from error during fetch\n            log_warning(f\"Warning: Fetch was interrupted by a transient error, review results for fetch completeness.\")\n        else:\n            # Save new checkpoint if entity_count is greater than one\n            if entity_count > 0:\n                helper.save_check_point(checkpoint_name, datetime.datetime.now().timestamp())\n", "customized_options": [{"name": "api_host", "value": "https://10.20.0.103"}, {"name": "entity_type", "value": "devices"}, {"name": "saved_query", "value": "Windows Servers"}, {"name": "page_size", "value": "10"}, {"name": "standoff_ms", "value": "0"}, {"name": "dynamic_field_mapping", "value": "{\"hostname\": \"host\", \"network_interfaces.ips\": \"ip_address\"}"}, {"name": "shorten_field_names", "value": false}, {"name": "incremental_data_ingest", "value": false}, {"name": "enforce_ssl_validation", "value": false}, {"name": "enable_include_details", "value": false}, {"name": "ssl_certificate_path", "value": ""}], "uuid": "0f90837d52134fc986c46663f97d5e3a", "sample_count": "41413"}]}, "field_extraction_builder": {"axonius_saved_query": {"is_parsed": true, "data_format": "json"}}, "global_settings_builder": {"global_settings": {"log_settings": {"log_level": "DEBUG"}, "customized_settings": [{"required": false, "name": "api_key", "label": "API Key", "placeholder": "", "default_value": "", "help_string": "The API Key from https://axonius.example.com/account -> API Key", "type": "password", "format_type": "password", "value": ""}, {"required": false, "name": "api_secret", "label": "API Secret", "placeholder": "", "default_value": "", "help_string": "The API Secret from https://axonius.example.com/account -> API Key", "type": "password", "format_type": "password", "value": ""}, {"required": false, "name": "https_request_timeout", "label": "https Request Timeout", "default_value": "900", "placeholder": "", "help_string": "How many seconds before a request timesout to the Axonius Host", "type": "text", "format_type": "text", "value": "900"}]}}, "sourcetype_builder": {"axonius_saved_query": {"metadata": {"event_count": 0, "data_input_name": "axonius_saved_query", "extractions_count": 0, "cims_count": 0}}}, "validation": {"validators": ["best_practice_validation", "data_model_mapping_validation", "field_extract_validation", "app_cert_validation"], "status": "job_finished", "validation_id": "v_1670347203_92", "progress": 1.0}}